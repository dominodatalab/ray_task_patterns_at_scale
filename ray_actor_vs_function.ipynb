{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92167aa-db81-47e6-9b35-1e0ffb62d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "\n",
    "def initialize_ray():\n",
    "    ray.shutdown()\n",
    "    \n",
    "    if ray.is_initialized() == False:\n",
    "       service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "       service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "       _temp_dir='/domino/datasets/local/{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "       #ray.util.connect(f\"{service_host}:{service_port}\")\n",
    "       address=f\"ray://{service_host}:{service_port}\"\n",
    "       ray.init(address=address, _temp_dir=_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed7ef8-0c84-4842-b058-672b9ccdd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "initialize_ray()\n",
    "\n",
    "# ---- CONFIG ----\n",
    "USE_ACTOR = True     # Set to False to use Ray function\n",
    "NUM_EVALS = 200      # Simulate Ray Tune sending 200 eval requests\n",
    "SLEEP_TIME = 0.1     # Simulate work\n",
    "ARRAY_SIZE_MB = 50   # Simulate memory pressure\n",
    "\n",
    "# ---- MEMORY PRESSURE OBJECT ----\n",
    "def make_big_array():\n",
    "    return np.ones((ARRAY_SIZE_MB * 250_000,), dtype=np.float32)  # ~50 MB\n",
    "\n",
    "# ---- ACTOR VERSION ----\n",
    "@ray.remote\n",
    "class EvaluatorActor:\n",
    "    def __init__(self):\n",
    "        self.cache = []  # Persistent memory (grows over time)\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        arr = make_big_array()\n",
    "        self.cache.append(arr)  # Causes memory accumulation\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        return x * 2\n",
    "\n",
    "# ---- FUNCTION VERSION ----\n",
    "@ray.remote\n",
    "def evaluate_fn(x):\n",
    "    arr = make_big_array()  # Ephemeral\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    return x * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe44ae-ea64-4c46-9ee7-d047a4c98132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment when you want to not use actors\n",
    "#USE_ACTOR = False\n",
    "# ---- DISPATCH ----\n",
    "print(f\"Use Actor? {USE_ACTOR}\")\n",
    "if USE_ACTOR:\n",
    "    actors = [EvaluatorActor.remote() for _ in range(10)]  # Fewer due to higher cost\n",
    "    futures = [actors[i % len(actors)].evaluate.remote(i) for i in range(NUM_EVALS)]\n",
    "else:\n",
    "    futures = [evaluate_fn.remote(i) for i in range(NUM_EVALS)]\n",
    "\n",
    "results = ray.get(futures)\n",
    "print(\"Sample results:\", results[:5])\n",
    "print(\"Total results count:\",len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a8219-ffdc-410d-b1c1-0d2021331f84",
   "metadata": {},
   "source": [
    "## What is going on?\n",
    "\n",
    "Ray actors fail unexpectedly in scenarios like Ray Tune–to–Eval pipelines because of the **accumulation of long-lived state, memory leaks, or scheduling contention**, \n",
    "none of which affect stateless Ray functions the same way.\n",
    "\n",
    "### Memory leak via Actor Internal State\n",
    "```python\n",
    "self.cache.append(arr)\n",
    "```\n",
    "- Leak: Each evaluate call creates a large array (~50 MB) and stores it in self.cache.\n",
    "\n",
    "- Since the actor lives across all calls, self.cache grows linearly with each call.\n",
    "\n",
    "- No deletion, no eviction, and Python’s GC won't clean up because references are held.\n",
    "\n",
    "- Ray cannot reclaim this memory since it's inside actor-managed state, not the object store.\n",
    "\n",
    "- Over time, this causes unbounded memory growth - a classic application-level memory leak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990b773-8d51-4741-a677-b454b2ff5fa7",
   "metadata": {},
   "source": [
    "### In contrast (Ray Function Version):\n",
    "```python\n",
    "evaluate_fn.remote(i)\n",
    "```\n",
    "- Each task runs, completes, and memory is released.\n",
    "\n",
    "- No retained state, no scheduler blocking beyond what’s necessary.\n",
    "\n",
    "- Ray can schedule tasks dynamically and aggressively without saturation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14091c-f0bb-4dd3-8a9b-ae6cdd371823",
   "metadata": {},
   "source": [
    "### What is queue saturation\n",
    "\n",
    "Queue saturation in Ray means the internal task or method call queue—maintained by the Raylet scheduler and GCS—has more pending tasks than it can handle efficiently, \n",
    "leading to stalled progress or failure to schedule new work.\n",
    "\n",
    "In the Actor Case:\n",
    "- You have 10 actors.\n",
    "\n",
    "- You submit 200 method calls.\n",
    "\n",
    "- Each actor can only execute one method at a time (unless max_concurrency is set).\n",
    "\n",
    "- So:\n",
    "\n",
    " - 10 calls begin executing.\n",
    "\n",
    " - 190 calls are placed in method call queues (per actor or at the Raylet).\n",
    "\n",
    "- These queues saturate:\n",
    "\n",
    "  - Memory usage grows (each queued call holds metadata and serialized args).\n",
    "\n",
    "  - Raylet may stall or delay new scheduling decisions.\n",
    "\n",
    "  - If the actor crashes or memory exceeds limits, pending calls are dropped or retried, increasing pressure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ca733-3c45-4cc1-891b-2282ffebf425",
   "metadata": {},
   "source": [
    "###  In the 200,000 Tasks Case:\n",
    "- 200,000 Ray tasks are submitted at once.\n",
    "\n",
    "- The driver floods the head node’s scheduler.\n",
    "\n",
    "- Ray reaches internal limits (max_in_flight_requests, network buffers, GCS table size).\n",
    "\n",
    "- Result: system hangs, freezes, or drops tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb1803-ff02-40c1-806c-6a8ae06b1eaa",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "\n",
    "| Feature              | Actor Queue Saturation                       | Task Flood Saturation                        |\n",
    "|----------------------|----------------------------------------------|----------------------------------------------|\n",
    "| Source of Saturation | Too many pending actor method calls          | Too many submitted tasks at once             |\n",
    "| Queued in            | Per-actor queues / Raylet scheduler          | Head node GCS + Raylet                       |\n",
    "| Resource block       | Limited actor concurrency                    | Global CPU/memory task limits                |\n",
    "| Failure mode         | Actor crashes, timeouts                      | Driver freeze, dashboard hangs               |\n",
    "| Mitigation           | Control method rate, cleanup state           | Throttle task submission, use `ray.wait()`   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba7bc8b-9504-406f-b3b1-43bb56f392c2",
   "metadata": {},
   "source": [
    "### Simply managing memory in actor state is not enough?\n",
    "\n",
    "Managing memory alone is not sufficient in the actor case. You must also address concurrency and scheduling contention, or the system will still degrade or stall.\n",
    "\n",
    "| Problem Type           | Cause                                              | Result                                      |\n",
    "|------------------------|----------------------------------------------------|---------------------------------------------|\n",
    "| Concurrency bottleneck | Actors run one method at a time by default         | Queues build up; calls wait indefinitely     |\n",
    "| Scheduling contention  | Too many method calls queued; Raylet saturated     | New calls delayed or dropped                |\n",
    "| Resource exhaustion    | Each actor holds CPU/memory for entire lifetime    | System can't schedule even idle work        |\n",
    "| Backpressure buildup   | Client or scheduler can’t dispatch fast enough     | Control plane stalls                         |\n",
    "\n",
    "\n",
    "Required Fixes (Cumulative, Not Isolated): \n",
    "1. **Bounded Submission**\n",
    "   Use ray.wait() or controlled loops to limit in-flight method calls.\n",
    "\n",
    "2. **Actor Memory Hygiene**\n",
    "   Explicitly free memory (del, gc.collect()), avoid large retained state.\n",
    "\n",
    "3. **Actor Pooling or Recycling**\n",
    "   Use a pool of short-lived actors or periodically destroy/recreate them.\n",
    "\n",
    "4. **Tune max_concurrency**\n",
    "   Set @ray.remote(max_concurrency=N) if methods are async-safe.\n",
    "\n",
    "5. **System Monitoring**\n",
    "   Use dashboard, ray memory, and logs to catch bottlenecks early.\n",
    "\n",
    "Actors introduce control-plane load, resource pinning, and queue buildup risks. Managing memory prevents crashes, but only concurrency-aware submission and system-aware orchestration prevent hangs.\n",
    "\n",
    "\n",
    "**Control-plane** load refers to the overhead of managing task scheduling, metadata, object references, and actor lifecycle, not the actual computation, but the orchestration of it. It’s the management traffic and coordination cost inside Ray.\n",
    "\n",
    "| Concept               | Description                                                                                   |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Control-plane load    | Cost of managing tasks, actors, object refs, dependencies, scheduling decisions, and metadata |\n",
    "| Head node saturation  | When the head node’s CPU, memory, or I/O is overwhelmed—often due to too much control-plane activity |\n",
    "\n",
    "Key Distinctions:\n",
    "\n",
    "| Aspect      | Control-plane Load                                 | Head Node Saturation                                  |\n",
    "|-------------|----------------------------------------------------|--------------------------------------------------------|\n",
    "| Scope       | Logical overhead: task metadata, scheduling        | Physical resource exhaustion (CPU, RAM, etc.)          |\n",
    "| Causes      | Too many tasks, actors, object refs                | Too much control-plane + other loads combined          |\n",
    "| Location    | GCS, Raylet, client-server comms                   | Entire head node (OS-level saturation)                 |\n",
    "| Symptoms    | Slow scheduling, delayed actor init                | Dashboard hangs, node unresponsive                     |\n",
    "| Mitigation  | Throttle submissions, batch, minimize state        | Distribute load, autoscale, reduce GCS pressure        |\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Submitting 200,000 tasks at once causes high control-plane load.\n",
    "\n",
    "If this overwhelms the Raylet or GCS on the head node, it leads to head node saturation.\n",
    "\n",
    "Control-plane load leading to Head node saturation is a causal chain, not two isolated issues. Saturation is the physical manifestation of excessive logical overhead.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88a7c2-66fa-483e-879e-73fdf8aca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mitigation use ray.wait()\n",
    "\n",
    "import ray\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ray.init()  # use your actual initialize_ray() if needed\n",
    "\n",
    "# ---- CONFIG ----\n",
    "USE_ACTOR = True\n",
    "NUM_EVALS = 200_000\n",
    "SLEEP_TIME = 0.1\n",
    "ARRAY_SIZE_MB = 50\n",
    "MAX_IN_FLIGHT = 1000  # throttle limit\n",
    "\n",
    "# ---- MEMORY PRESSURE OBJECT ----\n",
    "def make_big_array():\n",
    "    return np.ones((ARRAY_SIZE_MB * 250_000,), dtype=np.float32)\n",
    "\n",
    "@ray.remote\n",
    "class EvaluatorActor:\n",
    "    def __init__(self):\n",
    "        self.cache = []\n",
    "    def evaluate(self, x):\n",
    "        arr = make_big_array()\n",
    "        self.cache.append(arr)\n",
    "        time.sleep(SLEEP_TIME)\n",
    "        return x * 2\n",
    "\n",
    "@ray.remote\n",
    "def evaluate_fn(x):\n",
    "    arr = make_big_array()\n",
    "    time.sleep(SLEEP_TIME)\n",
    "    return x * 2\n",
    "\n",
    "# ---- DISPATCH ----\n",
    "print(f\"Use Actor? {USE_ACTOR}\")\n",
    "\n",
    "if USE_ACTOR:\n",
    "    actors = [EvaluatorActor.remote() for _ in range(10)]\n",
    "    def submit_task(i):\n",
    "        return actors[i % len(actors)].evaluate.remote(i)\n",
    "else:\n",
    "    def submit_task(i):\n",
    "        return evaluate_fn.remote(i)\n",
    "\n",
    "# ---- THROTTLED SUBMISSION ----\n",
    "pending = [submit_task(i) for i in range(min(MAX_IN_FLIGHT, NUM_EVALS))]\n",
    "results = []\n",
    "next_task_id = len(pending)\n",
    "\n",
    "while pending:\n",
    "    done, pending = ray.wait(pending, num_returns=1)\n",
    "    results.extend(ray.get(done))\n",
    "    if next_task_id < NUM_EVALS:\n",
    "        pending.append(submit_task(next_task_id))\n",
    "        next_task_id += 1\n",
    "\n",
    "print(\"Sample results:\", results[:5])\n",
    "print(\"Total results count:\", len(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
